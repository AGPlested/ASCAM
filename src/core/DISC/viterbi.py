# All credit for the DISC algorithm belongs to the authors of
# White, D.S., Goldschen-Ohm, M.P., Goldsmith, R.H., Chanda, B. Top-down machine learning approach for high-throughput single-molecule analysis. Elife 2020, 9
# The code in this module is based on their paper and partly on their
# matlab implementation at commit af19eae on https://github.com/ChandaLab/DISC/

import numpy as np
from scipy.stats import norm

from .utils import normal_pdf

def compute_emission_matrix(observations, components):
    """
    Compute the probability of each observation being generated by each
    state.
    Input:
     observations = N×1 array
     components = 3×K array
    N = Length of observations
    K = Number of states
    """
    n_obs = len(observations)
    n_states = np.shape(components)[1]
    emission_matrix = np.zeros((n_states, n_obs))
    for i in range(n_states):
        mu = components[1,i]
        sigma = components[2,i]
        emission_matrix[i,:] = normal_pdf(observations, mu=mu, sigma=sigma)
    # Return the normalized matrix
    return emission_matrix/np.sum(emission_matrix, axis=1, keepdims=True)

def empirical_transition_matrix(data_fit):
    """
    Compute the transition matrix of a Markov chain based on a
    realization thereof.
    Input:
     data_fit = fit sorting the data into a small number of states
    N = Length of sample
    K = Number of states
    """
    states = np.unique(data_fit)
    n_states = len(states)
    # Convert data_fit into a trajectory in which the states are
    # represented by integers.
    trajectory = np.zeros(np.shape(data_fit), int)
    for (i,s) in enumerate(states):
        trajectory[data_fit==s] = i
    transition_matrix = np.zeros((n_states, n_states))
    for i in range(len(trajectory)-1):
        transition_matrix[trajectory[i], trajectory[i+1]] += 1
    # If any states are never exited add small noise to the matrix to avoid
    # diving by zero.
    if np.any(np.sum(transition_matrix, axis=1)==0):
        transition_matrix += 1e-6
    # Return the normalized matrix
    return transition_matrix/np.sum(transition_matrix, axis=1,
                                    keepdims=True)

def viterbi_path(initial_dist, transition_matrix,
                 emission_matrix, state_vals=None):
    (K,N) = np.shape(emission_matrix)
    # probability of data point belonging to each state
    state_prob = np.zeros((K,N))
    # State assignment of each data point per state
    predecessor = np.zeros((K,N), dtype=int)
    # scale = np.zeros(1,N)  # 1 / total probabilities of all states
    # the `scale` matrix can be used to calculate the log likelihood of
    # the Viterbi path
    # Initialization: Determine the most likely state of data point 0
    state_prob[:,0] = initial_dist * emission_matrix[:,0]
    # scale[0] = 0/sum(state_prob[:,0]);
    # normalize values to sum to 1
    state_prob[:,0] = state_prob[:,0] / np.sum(state_prob[:,0])
    # Set predecessor of initial state to arbitrary value, since there is
    # no predecessor to n=0
    predecessor[:,0] = 0
    for n in range(1, N):
        M = state_prob[:, n-1] * transition_matrix.T
        predecessor[:, n] = np.argmax(M, axis=1)
        state_prob[:, n] = M[range(K), predecessor[:, n]]*emission_matrix[:, n]
        # scale[n] = 1/sum(state_prob[:,n]);
        # normalize
        state_prob[:, n] = state_prob[:,n] / np.sum(state_prob[:,n])
    # Find Most probable state path
    path = np.zeros(N, dtype=int)
    # loop backwards from data points N-1 to end
    path[-1] = np.argmax(state_prob[:,-1])  # Last data point
    for n in range(N-2,-1,-1):
        path[n] = predecessor[path[n+1],n+1]
    # log_likelihood = -sum(log(scale));
    # Assign values to states (if state values are given).
    if state_vals is not None:
        path = path.astype(float)
        for (i, s) in enumerate(state_vals):
            path[path==i] = s
    return path

def viterbi_path_from_data(data, data_fit):
    """
    Run Viterbi algorithm to find best fit to the given number of
    states and their amplitudes.
    Use empirical distribution of states as initial distribution for the
    Viterbi algorithm. Compute best fit normal distribution for
    emission matrix approximation.
    """
    states = np.unique(data_fit)
    n_states = len(states)
    components = np.zeros((3, n_states))
    for (i,s) in enumerate(states):
        state_indices = data_fit==s
        components[0, i] = np.sum(state_indices)
        components[1, i], components[2,i] = norm.fit(data[state_indices])
    components[0, :] /= len(data)
    EM = compute_emission_matrix(data, components)
    TM = empirical_transition_matrix(data_fit)
    return viterbi_path(components[0,:], TM, EM, state_vals=states)
